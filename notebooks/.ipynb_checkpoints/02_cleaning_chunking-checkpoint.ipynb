{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 02: Cleaning and Chunking\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Goal\\n\",\n",
    "    \"Clean text data and create semantic chunks for vector embedding.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 1: Imports\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Libraries imported\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 2: Load raw documents\\n\",\n",
    "    \"raw_docs = []\\n\",\n",
    "    \"for file in os.listdir(\\\"../data/raw\\\"):\\n\",\n",
    "    \"    if file.endswith(\\\".json\\\") and file != \\\"summary.json\\\":\\n\",\n",
    "    \"        with open(f\\\"../data/raw/{file}\\\", \\\"r\\\") as f:\\n\",\n",
    "    \"            raw_docs.append(json.load(f))\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"üìö Loaded {len(raw_docs)} documents\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 3: Text cleaning function\\n\",\n",
    "    \"def clean_text(text):\\n\",\n",
    "    \"    \\\"\\\"\\\"Clean and normalize text\\\"\\\"\\\"\\n\",\n",
    "    \"    # Lowercase\\n\",\n",
    "    \"    text = text.lower()\\n\",\n",
    "    \"    # Remove extra whitespace\\n\",\n",
    "    \"    text = re.sub(r'\\\\s+', ' ', text)\\n\",\n",
    "    \"    # Remove special characters but keep basic punctuation\\n\",\n",
    "    \"    text = re.sub(r'[^\\\\w\\\\s.,!?-]', '', text)\\n\",\n",
    "    \"    # Normalize units\\n\",\n",
    "    \"    text = re.sub(r'(\\\\d+)\\\\s*(mmhg|bpm|hr|ms)', r'\\\\1 \\\\2', text)\\n\",\n",
    "    \"    return text.strip()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test cleaning\\n\",\n",
    "    \"test_text = \\\"Blood pressure   should be below 140/90 mmHg!!!\\\"\\n\",\n",
    "    \"print(f\\\"Test cleaning: {clean_text(test_text)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 4: Chunking function\\n\",\n",
    "    \"def chunk_text(text, chunk_size=300, overlap=50):\\n\",\n",
    "    \"    \\\"\\\"\\\"Split text into overlapping chunks\\\"\\\"\\\"\\n\",\n",
    "    \"    words = text.split()\\n\",\n",
    "    \"    chunks = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(words) <= chunk_size:\\n\",\n",
    "    \"        return [\\\" \\\".join(words)]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i in range(0, len(words), chunk_size - overlap):\\n\",\n",
    "    \"        chunk = \\\" \\\".join(words[i:i + chunk_size])\\n\",\n",
    "    \"        chunks.append(chunk)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Stop if we've reached the end\\n\",\n",
    "    \"        if i + chunk_size >= len(words):\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"            \\n\",\n",
    "    \"    return chunks\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test chunking\\n\",\n",
    "    \"test_long_text = \\\" \\\".join([\\\"word\\\" + str(i) for i in range(500)])\\n\",\n",
    "    \"test_chunks = chunk_text(test_long_text, chunk_size=100, overlap=20)\\n\",\n",
    "    \"print(f\\\"Test chunking created {len(test_chunks)} chunks\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 5: Process all documents\\n\",\n",
    "    \"cleaned_chunks = []\\n\",\n",
    "    \"chunk_id = 0\\n\",\n",
    "    \"\\n\",\n",
    "    \"for doc in raw_docs:\\n\",\n",
    "    \"    cleaned_text = clean_text(doc[\\\"text\\\"])\\n\",\n",
    "    \"    chunks = chunk_text(cleaned_text, chunk_size=300, overlap=50)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for chunk in chunks:\\n\",\n",
    "    \"        cleaned_chunks.append({\\n\",\n",
    "    \"            \\\"chunk_id\\\": f\\\"chunk_{chunk_id}\\\",\\n\",\n",
    "    \"            \\\"source\\\": doc[\\\"source\\\"],\\n\",\n",
    "    \"            \\\"title\\\": doc[\\\"title\\\"],\\n\",\n",
    "    \"            \\\"text\\\": chunk,\\n\",\n",
    "    \"            \\\"word_count\\\": len(chunk.split())\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"        chunk_id += 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úÖ Created {len(cleaned_chunks)} chunks from {len(raw_docs)} documents\\\")\\n\",\n",
    "    \"print(f\\\"Average chunk length: {sum([c['word_count'] for c in cleaned_chunks])/len(cleaned_chunks):.0f} words\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 6: Save cleaned chunks\\n\",\n",
    "    \"with open(\\\"../data/cleaned/chunks.json\\\", \\\"w\\\") as f:\\n\",\n",
    "    \"    json.dump(cleaned_chunks, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Also save as CSV for easy viewing\\n\",\n",
    "    \"df_chunks = pd.DataFrame(cleaned_chunks)\\n\",\n",
    "    \"df_chunks.to_csv(\\\"../data/cleaned/chunks.csv\\\", index=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüìä Chunk Statistics:\\\")\\n\",\n",
    "    \"print(df_chunks['source'].value_counts())\\n\",\n",
    "    \"print(f\\\"\\\\n‚úÖ Saved {len(cleaned_chunks)} chunks to data/cleaned/chunks.json and chunks.csv\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 7: Preview chunks\\n\",\n",
    "    \"print(\\\"\\\\nüîç Sample Chunks:\\\")\\n\",\n",
    "    \"for i, chunk in enumerate(cleaned_chunks[:3]):\\n\",\n",
    "    \"    print(f\\\"\\\\nChunk {i+1} ({chunk['source']} - {chunk['title']}):\\\")\\n\",\n",
    "    \"    print(f\\\"{chunk['text'][:150]}...\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.12.7\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
